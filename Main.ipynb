{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/194 [00:00<?, ?it/s]\n",
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\arthur\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\arthur\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 148, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\arthur\\Anaconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already droped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏                                      | 1/194 [01:43<5:31:31, 103.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already droped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 194/194 [6:08:57<00:00, 114.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "root_path = \"csv/traffic\"\n",
    "drop = ['samples_below_100pct_ff', 'samples_below_95pct_ff', 'samples_below_90pct_ff', 'samples_below_80pct_ff.1',\n",
    "          'samples_below_80pct_ff', 'samples_below_75pct_ff', 'samples_below_70pct_ff', 'samples_below_65pct_ff',\n",
    "          'samples_below_60pct_ff', 'samples_below_55pct_ff', 'samples_below_50pct_ff', 'samples_below_45pct_ff',\n",
    "          'samples_below_40pct_ff', 'samples_below_35pct_ff', 'samples_below_30pct_ff', 'samples_below_25pct_ff',\n",
    "          'samples_below_20pct_ff', 'samples_below_15pct_ff', 'samples_below_10pct_ff', 'samples_below_5pct_ff']\n",
    "\n",
    "\n",
    "files = os.listdir(root_path)\n",
    "for f in tqdm(files):\n",
    "    df = pd.read_csv('{}/{}'.format(root_path, f))\n",
    "    try:\n",
    "        df.drop(columns=drop, inplace=True)\n",
    "        df.to_csv('{}/{}'.format(root_path, f))\n",
    "    except ValueError:\n",
    "        print('Already droped')  ...................................-      \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2.017846e+06\n",
      "mean     1.008922e+06\n",
      "std      5.825021e+05\n",
      "min      0.000000e+00\n",
      "25%      5.044612e+05\n",
      "50%      1.008922e+06\n",
      "75%      1.513384e+06\n",
      "max      2.017845e+06\n",
      "Name: Unnamed: 0, dtype: float64\n",
      "----------\n",
      "count                 2017846\n",
      "unique                    288\n",
      "top       03/03/2018 21:10:00\n",
      "freq                     9576\n",
      "Name: utc_time_id, dtype: object\n",
      "----------\n",
      "count     2017846\n",
      "unique          2\n",
      "top           tmc\n",
      "freq      1609313\n",
      "Name: source_ref, dtype: object\n",
      "----------\n",
      "count       2017846\n",
      "unique        30141\n",
      "top       106+23194\n",
      "freq            288\n",
      "Name: source_id, dtype: object\n",
      "----------\n",
      "count       2017846\n",
      "unique            1\n",
      "top       hereA0106\n",
      "freq        2017846\n",
      "Name: feed_id, dtype: object\n",
      "----------\n",
      "count    2.017846e+06\n",
      "mean     9.866922e-01\n",
      "std      1.145891e-01\n",
      "min      0.000000e+00\n",
      "25%      1.000000e+00\n",
      "50%      1.000000e+00\n",
      "75%      1.000000e+00\n",
      "max      1.000000e+00\n",
      "Name: primary_link_source_flag, dtype: float64\n",
      "----------\n",
      "count    2.017846e+06\n",
      "mean     5.000703e+00\n",
      "std      5.958459e-02\n",
      "min      3.000000e+00\n",
      "25%      5.000000e+00\n",
      "50%      5.000000e+00\n",
      "75%      5.000000e+00\n",
      "max      1.000000e+01\n",
      "Name: samples, dtype: float64\n",
      "----------\n",
      "count    2.012521e+06\n",
      "mean     5.599094e+01\n",
      "std      2.462219e+01\n",
      "min      3.310000e+00\n",
      "25%      3.753000e+01\n",
      "50%      5.058000e+01\n",
      "75%      7.092000e+01\n",
      "max      1.299900e+02\n",
      "Name: avg_speed, dtype: float64\n",
      "----------\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: avg_flow, dtype: float64\n",
      "----------\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: avg_occ, dtype: float64\n",
      "----------\n",
      "count    2.017846e+06\n",
      "mean     6.516734e+01\n",
      "std      2.182340e+01\n",
      "min      1.400000e+01\n",
      "25%      4.921000e+01\n",
      "50%      5.986000e+01\n",
      "75%      7.702000e+01\n",
      "max      1.140100e+02\n",
      "Name: avg_freeflow_speed, dtype: float64\n",
      "----------\n",
      "count    2.012521e+06\n",
      "mean     1.790117e+00\n",
      "std      2.740035e+00\n",
      "min      0.000000e+00\n",
      "25%      2.500000e-01\n",
      "50%      1.120000e+00\n",
      "75%      2.270000e+00\n",
      "max      8.600000e+01\n",
      "Name: avg_travel_time, dtype: float64\n",
      "----------\n",
      "count    2.017846e+06\n",
      "mean     8.713955e-01\n",
      "std      1.789200e+00\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      1.000000e+01\n",
      "Name: high_quality_samples, dtype: float64\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for col in df:\n",
    "    print(df[col].describe())\n",
    "    print('----------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column analyzing\n",
    "Analyzing each columns in the data frame, it turned clear that only 3 columns is from our interrest: **utc_time_id, source_id and avg_speed**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## source_id\n",
    "\n",
    "The column source_id give us 30141 unique values, refering to 30141 diferrent measurement points through the city in question(San Francisco), futher our dataframe will have the dimensionalite of ** [N, 30141]**, where N is the number of samples, and each column contains the avg_speed of a specific measurement point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>utc_time_id</th>\n",
       "      <th>source_ref</th>\n",
       "      <th>source_id</th>\n",
       "      <th>feed_id</th>\n",
       "      <th>primary_link_source_flag</th>\n",
       "      <th>samples</th>\n",
       "      <th>avg_speed</th>\n",
       "      <th>avg_flow</th>\n",
       "      <th>avg_occ</th>\n",
       "      <th>avg_freeflow_speed</th>\n",
       "      <th>avg_travel_time</th>\n",
       "      <th>high_quality_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>03/03/2018 07:30:00</td>\n",
       "      <td>tmc</td>\n",
       "      <td>106-13503</td>\n",
       "      <td>hereA0106</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>49.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>03/03/2018 07:35:00</td>\n",
       "      <td>tmc</td>\n",
       "      <td>106-13503</td>\n",
       "      <td>hereA0106</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>03/03/2018 07:40:00</td>\n",
       "      <td>tmc</td>\n",
       "      <td>106-13503</td>\n",
       "      <td>hereA0106</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>03/03/2018 07:45:00</td>\n",
       "      <td>tmc</td>\n",
       "      <td>106-13503</td>\n",
       "      <td>hereA0106</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>03/03/2018 07:50:00</td>\n",
       "      <td>tmc</td>\n",
       "      <td>106-13503</td>\n",
       "      <td>hereA0106</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          utc_time_id source_ref  source_id    feed_id  \\\n",
       "0           0  03/03/2018 07:30:00        tmc  106-13503  hereA0106   \n",
       "1           1  03/03/2018 07:35:00        tmc  106-13503  hereA0106   \n",
       "2           2  03/03/2018 07:40:00        tmc  106-13503  hereA0106   \n",
       "3           3  03/03/2018 07:45:00        tmc  106-13503  hereA0106   \n",
       "4           4  03/03/2018 07:50:00        tmc  106-13503  hereA0106   \n",
       "\n",
       "   primary_link_source_flag  samples  avg_speed  avg_flow  avg_occ  \\\n",
       "0                         1        5       49.8       NaN      NaN   \n",
       "1                         1        5       50.0       NaN      NaN   \n",
       "2                         1        5       50.0       NaN      NaN   \n",
       "3                         1        5       50.0       NaN      NaN   \n",
       "4                         1        5       50.0       NaN      NaN   \n",
       "\n",
       "   avg_freeflow_speed  avg_travel_time  high_quality_samples  \n",
       "0                50.0             0.96                     0  \n",
       "1                50.0             0.96                     0  \n",
       "2                50.0             0.96                     0  \n",
       "3                50.0             0.96                     0  \n",
       "4                50.0             0.96                     0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time column\n",
    "The time utc_time_column is in string format, not in datetime format, it is needed to convert for easier use in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/194 [00:00<?, ?it/s]\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\arthur\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\arthur\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 148, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\arthur\\Anaconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|█████████████████████████████████████| 194/194 [5:49:40<00:00, 108.15s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for f in tqdm(files):\n",
    "    df = pd.read_csv('{}/{}'.format(root_path, f))\n",
    "    df = df[['utc_time_id', 'source_id', 'avg_speed']]\n",
    "    df['utc_time_id'] = pd.to_datetime(df['utc_time_id'], infer_datetime_format=True)\n",
    "    df.to_csv('{}/{}'.format(root_path, f), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File manager\n",
    "\n",
    "Due the high volume of information that needs to be process, we will create a way to process the data in multiprocess and in diferents computers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "from queue import Queue\n",
    "import random\n",
    "\n",
    "class FileManager(object):\n",
    "    def __init__(self, directory = 'csv/traffic', path = 'C:/Users/arthur/Dropbox/TrafficFlow/lista.pkl'):\n",
    "\n",
    "        self.files = os.listdir(directory)\n",
    "        self.done = []\n",
    "        self.processing = []\n",
    "        self.path = path\n",
    "        \n",
    "    def get_item(self):\n",
    "        x = self.files.pop(random.randint(0,len(self.files)-1))\n",
    "        self.processing.append(x)\n",
    "        save_pkl(self, self.path)\n",
    "        return x\n",
    "    def finnish(self, x):\n",
    "        self.processing.remove(x)\n",
    "        self.done.append(x)\n",
    "        save_pkl(self, self.path)\n",
    "        \n",
    "    def set_path(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def __str__(self):\n",
    "        msg = '{} itens to be process \\n {} itens processed'.format(len(self.files), len(self.processing) + len(self.done))\n",
    "        return msg\n",
    "    \n",
    "def save_pkl(obj, path = 'C:/Users/arthur/Dropbox/TrafficFlow/lista.pkl'):\n",
    "    pkl.dump(obj, open(path, 'wb'))\n",
    "    \n",
    "def load_pkl(path = 'C:/Users/arthur/Dropbox/TrafficFlow/lista.pkl'):\n",
    "    obj = pkl.load(open(path, 'rb'))\n",
    "    if len(obj.processing) > 0:\n",
    "        print('The process was stoped with running process')\n",
    "        print('Restoring process to process queue')\n",
    "        obj.files += obj.processing\n",
    "        obj.processing = []\n",
    "    return obj\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropdox_path = 'C:/Users/arthur/Dropbox/TrafficFlow/lista.pkl'\n",
    "dir_path = 'csv/traffic'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_date(file):\n",
    "    name, ext = file.rsplit('.')\n",
    "    return name[-10:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess(file):\n",
    "    df = pd.read_csv(file, dtype={'source_id':'category'})\n",
    "    df.set_index('utc_time_id', inplace=True)\n",
    "    g = df.groupby('source_id') \n",
    "    path = 'csv/traffic/{}.csv'.format(get_date(file))\n",
    "    print(path)\n",
    "    col =[]\n",
    "    for name, group in tqdm(g):\n",
    "        col.append(name)\n",
    "        try:      \n",
    "            df_res = pd.concat([df_res, group['avg_speed']], axis=1)\n",
    "        except NameError:\n",
    "            df_res = group['avg_speed']\n",
    "    df_res.columns = col\n",
    "    df_res.to_csv(path, index=True)\n",
    "    return file\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Task(object):\n",
    "    def __init__(self, file, user, status):\n",
    "        self.file = file\n",
    "        self.user = user\n",
    "        self.status = status \n",
    "        \n",
    "\n",
    "    def get_file(self):\n",
    "        self.status = 'running'\n",
    "        return self.file\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '{} - {}'.format(self.user, self.file)\n",
    "        \n",
    "def get_status(obj):\n",
    "    arthur_count = 0\n",
    "    zaroni_count = 0\n",
    "    tony_count = 0\n",
    "    total = len(obj)\n",
    "    for task in obj:\n",
    "        if task.status == 'done':\n",
    "            if task.user == 'arthur':\n",
    "                arthur_count += 1\n",
    "            elif task.user == 'zaroni':\n",
    "                zaroni_count += 1\n",
    "            else:\n",
    "                tony_count += 1\n",
    "    print('Arthur converted {}'.format(arthur_count))\n",
    "    print('Zaroni converted {}'.format(zaroni_count))\n",
    "    print('Tony converted {}'.format(tony_count))\n",
    "    print('{} still missing'.format(total - arthur_count - zaroni_count - tony_count))\n",
    "    print('Progress: {}/{}'.format((arthur_count+zaroni_count+tony_count),total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle as pkl \n",
    "direc = os.listdir(dir_path)\n",
    "\n",
    "create = False\n",
    "\n",
    "if create:\n",
    "    len_ = len(direc)\n",
    "    lista = []\n",
    "    for i, element in enumerate(direc):\n",
    "        if i < len_/3:\n",
    "            lista.append(Task('{}/{}'.format(dir_path, element), 'arthur', 'raw'))\n",
    "        elif i < len_*2/3:\n",
    "            lista.append(Task(element, 'Zaroni', 'raw'))\n",
    "        else:\n",
    "            lista.append(Task(element, 'Tony', 'raw'))\n",
    "\n",
    "    pkl.dump(lista, open(dropdox_path, 'wb'))\n",
    "else:\n",
    "    lista = pkl.load(open(dropdox_path, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/traffic/2017_10_01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 22870/22870 [52:42<00:00,  7.23it/s]\n"
     ]
    }
   ],
   "source": [
    "user = 'arthur'\n",
    "\n",
    "\n",
    "for l in lista:\n",
    "    if l.user == user and l.status == 'raw':\n",
    "        files_converted = preprocess(l.get_file())\n",
    "        l.status = 'done'\n",
    "        pkl.dump(lista, open(dropdox_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your name here\n",
    "from multiprocessing import freeze_support\n",
    "number_of_process = 4\n",
    "pool = multiprocessing.Pool(processes=number_of_process)\n",
    "with multiprocessing.Pool(3, ini) as p:\n",
    "    freeze_support()\n",
    "    files_converted = p.map(target = preprocess, [f.file for f in lista if ((f.user == user) and f.status != 'done' )])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'csv/traffic'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
